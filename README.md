# ğŸ¤– Dynamic Hand Gesture Recognition System

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org)
[![React](https://img.shields.io/badge/React-18.0+-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://reactjs.org)
[![Flask](https://img.shields.io/badge/Flask-2.0+-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.0+-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-Google-4285F4?style=for-the-badge&logo=google&logoColor=white)](https://mediapipe.dev)

> **Real-time gesture recognition system combining static and dynamic hand gestures with audio-visual learning features for enhanced sign language communication.**

## ğŸ¯ Overview

This comprehensive gesture recognition system leverages deep learning to detect and classify both static hand gestures (numbers, alphabets) and dynamic gestures (words, commands) in real-time. Built with a modern web interface, the system includes innovative features like speech-to-GIF mapping and visual learning capabilities, making it an effective tool for sign language education and accessibility.

## âœ¨ Key Features

### ğŸ–ï¸ **Dual Gesture Recognition**
- **Static Gestures**: Numbers (1-9) & Alphabets (A-Z) using ANN with 87 landmark features
- **Dynamic Gestures**: Words and commands using LSTM with face+hand+body landmarks (126+ features)

### ğŸŒ **Full-Stack Web Application**
- **Frontend**: React.js with real-time webcam integration
- **Backend**: Flask API for model inference and data processing
- **Real-time Predictions**: Live confidence scores and gesture classification

### ğŸµ **Audio-Visual Learning**
- **Speech Recognition**: Voice-to-text processing
- **GIF Database**: Mapped visual responses for enhanced learning
- **Multimodal Interface**: Gesture + Audio input for comprehensive communication

### ğŸ”¥ **Advanced ML Pipeline**
- **Computer Vision**: OpenCV + MediaPipe for robust landmark extraction
- **Deep Learning**: Custom ANN and LSTM architectures
- **Real-time Processing**: Optimized for low-latency predictions

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontendâ”‚    â”‚   Flask Backend â”‚    â”‚  ML Models      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Webcam Stream â”‚â—„â”€â”€â–ºâ”‚ â€¢ API Endpoints â”‚â—„â”€â”€â–ºâ”‚ â€¢ Static ANN    â”‚
â”‚ â€¢ Real-time UI  â”‚    â”‚ â€¢ Model Loading â”‚    â”‚ â€¢ Dynamic LSTM  â”‚
â”‚ â€¢ Audio Input   â”‚    â”‚ â€¢ GIF Mapping   â”‚    â”‚ â€¢ MediaPipe     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Interfaceâ”‚    â”‚   Data Pipeline â”‚    â”‚   CV Processing â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Predictions   â”‚    â”‚ â€¢ Preprocessing â”‚    â”‚ â€¢ Landmark Ext. â”‚
â”‚ â€¢ Confidence    â”‚    â”‚ â€¢ Augmentation  â”‚    â”‚ â€¢ Normalization â”‚
â”‚ â€¢ GIF Display   â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Feature Eng.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

### **Machine Learning & AI**
| Technology | Purpose | Version |
|------------|---------|---------|
| TensorFlow/Keras | Deep Learning Framework | 2.8+ |
| MediaPipe | Hand/Face/Body Landmark Detection | 0.8+ |
| OpenCV | Computer Vision Processing | 4.5+ |
| NumPy | Numerical Computing | 1.21+ |
| Scikit-learn | ML Utilities | 1.0+ |

### **Web Development**
| Technology | Purpose | Version |
|------------|---------|---------|
| React.js | Frontend Framework | 18.0+ |
| Flask | Backend API | 2.0+ |
| JavaScript/ES6 | Frontend Logic | - |
| HTML5/CSS3 | UI Structure & Styling | - |
| Axios | HTTP Client | 0.27+ |

### **Data & Storage**
| Technology | Purpose | Version |
|------------|---------|---------|
| JSON | Data Serialization | - |
| CSV | Dataset Storage | - |
| Local Storage | Model & GIF Storage | - |

## ğŸ“Š Model Performance

### **Static Gesture Recognition (ANN)**
- **Architecture**: Multi-layer Neural Network
- **Input Features**: 87 hand landmarks (MediaPipe)
- **Classes**: 35 (Numbers 1-9, Alphabets A-Z)
- **Accuracy**: ~92% on test set
- **Inference Time**: <10ms per prediction

### **Dynamic Gesture Recognition (LSTM)**
- **Architecture**: Bidirectional LSTM + Dense layers
- **Input Features**: 126+ (face+hand+body landmarks)
- **Sequence Length**: 20-30 frames
- **Classes**: 15+ dynamic gestures/words
- **Accuracy**: ~88% on validation set
- **Inference Time**: <50ms per sequence

## ğŸš€ Quick Start

### **Prerequisites**
```bash
# Python 3.8+
python --version

# Node.js 16+
node --version
npm --version
```

### **Installation**

1. **Clone the Repository**
```bash
git clone https://github.com/yourusername/gesture-recognition-system.git
cd gesture-recognition-system
```

2. **Backend Setup**
```bash
cd backend
pip install -r requirements.txt
python app.py
```

3. **Frontend Setup**
```bash
cd frontend
npm install
npm start
```

4. **Access the Application**
```
Frontend: http://localhost:3000
Backend API: http://localhost:5000
```

## ğŸ“‚ Project Structure

```
gesture-recognition-system/
â”œâ”€â”€ ğŸ“ frontend/                 # React.js web application
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/          # React components
â”‚   â”‚   â”œâ”€â”€ services/            # API services
â”‚   â”‚   â””â”€â”€ styles/              # CSS styling
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ ğŸ“ backend/                  # Flask API server
â”‚   â”œâ”€â”€ app.py                   # Main Flask application
â”‚   â”œâ”€â”€ models/                  # Trained ML models
â”‚   â”‚   â”œâ”€â”€ static_model.h5      # ANN for static gestures
â”‚   â”‚   â””â”€â”€ dynamic_model.h5     # LSTM for dynamic gestures
â”‚   â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”‚   â”œâ”€â”€ preprocessing.py     # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ landmark_extraction.py
â”‚   â”‚   â””â”€â”€ audio_processing.py
â”‚   â”œâ”€â”€ gifs_database/           # GIF files and mappings
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ data/                     # Dataset and training data
â”‚   â”œâ”€â”€ static/                  # Static gesture datasets
â”‚   â”œâ”€â”€ dynamic/                 # Dynamic gesture datasets
â”‚   â””â”€â”€ processed/               # Preprocessed data
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ static_training.ipynb    # Static model training
â”‚   â”œâ”€â”€ dynamic_training.ipynb   # Dynamic model training
â”‚   â””â”€â”€ data_analysis.ipynb      # Data exploration
â”‚
â”œâ”€â”€ ğŸ“ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ data_collection.py       # Data collection tools
â”‚   â”œâ”€â”€ model_training.py        # Training pipelines
â”‚   â””â”€â”€ evaluation.py            # Model evaluation
â”‚
â””â”€â”€ ğŸ“„ README.md                 # Project documentation
```

## ğŸ® Usage Guide

### **1. Real-time Gesture Recognition**
1. Click "Start Prediction" on the web interface
2. Allow camera permissions
3. Show static gestures (numbers/letters) or dynamic gestures (words)
4. View real-time predictions with confidence scores

### **2. Audio-to-GIF Learning**
1. Click the microphone button
2. Speak a word clearly
3. Watch as the system displays the corresponding GIF
4. Use for visual learning and sign language practice

### **3. Data Collection Mode**
1. Run the data collection script
2. Follow on-screen instructions for recording gestures
3. Ensure proper lighting and hand visibility
4. Collect diverse samples for robust training

## ğŸ§  Model Details

### **Static Gesture Model (ANN)**
```python
# Architecture Overview
Input Layer (87 features) 
    â†“
Dense Layer (128 neurons, ReLU)
    â†“
Dropout (0.3)
    â†“
Dense Layer (64 neurons, ReLU)
    â†“
Dropout (0.2)
    â†“
Output Layer (35 classes, Softmax)
```

### **Dynamic Gesture Model (LSTM)**
```python
# Architecture Overview
Input Layer (sequence_length, 126+ features)
    â†“
LSTM Layer (128 units, return_sequences=True)
    â†“
LSTM Layer (64 units)
    â†“
Dense Layer (64 neurons, ReLU)
    â†“
Dropout (0.3)
    â†“
Output Layer (num_classes, Softmax)
```

## ğŸ”¬ Training Process

### **Data Collection**
- **Static**: 50+ samples per class (35 classes)
- **Dynamic**: 30+ sequences per gesture (15+ gestures)
- **Augmentation**: Rotation, scaling, noise injection
- **Validation**: 80/20 train-test split

### **Training Configuration**
- **Optimizer**: Adam (lr=0.001)
- **Loss Function**: Categorical Crossentropy
- **Batch Size**: 32
- **Epochs**: 100 (with early stopping)
- **Regularization**: Dropout, L2 regularization

## ğŸŒŸ Innovations

### **1. Multi-Modal Landmark Detection**
- Traditional approach: Hand landmarks only
- **Our Innovation**: Face + Hand + Body landmarks for better reference points
- **Result**: Improved accuracy for dynamic gestures

### **2. Audio-Visual Integration**
- **Speech-to-GIF Mapping**: Voice commands trigger visual responses
- **Educational Focus**: Enhanced learning for sign language students
- **Accessibility**: Multi-sensory communication support

### **3. Real-time Web Interface**
- **Seamless Integration**: Direct webcam access in browser
- **Live Feedback**: Real-time confidence scores and predictions
- **User-Friendly**: One-click prediction activation

## ğŸ¯ Applications

- **ğŸ« Education**: Sign language learning and practice
- **â™¿ Accessibility**: Communication aid for hearing-impaired individuals
- **ğŸ® Gaming**: Gesture-based game controls
- **ğŸ  Smart Home**: Hands-free device control
- **ğŸ‘¨â€ğŸ’» Development**: Computer vision research and prototyping

## ğŸš€ Future Enhancements

- [ ] **Mobile App Development**: React Native implementation
- [ ] **Cloud Deployment**: AWS/GCP hosting with scalable infrastructure
- [ ] **Multi-Language Support**: International sign language variants
- [ ] **3D Gesture Recognition**: Depth camera integration
- [ ] **Edge Deployment**: TensorFlow Lite optimization
- [ ] **Collaborative Features**: Multi-user gesture sessions
- [ ] **Advanced Analytics**: Usage tracking and improvement suggestions

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the Repository**
2. **Create Feature Branch**: `git checkout -b feature/AmazingFeature`
3. **Commit Changes**: `git commit -m 'Add AmazingFeature'`
4. **Push to Branch**: `git push origin feature/AmazingFeature`
5. **Open Pull Request**

### **Development Guidelines**
- Follow PEP 8 for Python code
- Use ESLint for JavaScript code
- Add tests for new features
- Update documentation as needed

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **MediaPipe Team** for robust landmark detection
- **TensorFlow Community** for comprehensive ML framework
- **OpenCV Contributors** for computer vision tools
- **React Team** for modern web development capabilities

## ğŸ“ Contact

**Project Maintainer**: Computer Science Engineering Student  
**Institution**: India  
**Email**: [your.email@example.com]  
**LinkedIn**: [Your LinkedIn Profile]  
**GitHub**: [Your GitHub Profile]  

---

<div align="center">

### ğŸŒŸ If you found this project helpful, please give it a star! â­

[![GitHub stars](https://img.shields.io/github/stars/yourusername/gesture-recognition-system.svg?style=social&label=Star)](https://github.com/yourusername/gesture-recognition-system)
[![GitHub forks](https://img.shields.io/github/forks/yourusername/gesture-recognition-system.svg?style=social&label=Fork)](https://github.com/yourusername/gesture-recognition-system/fork)

**Made with â¤ï¸ for accessibility and innovation in human-computer interaction**

</div>